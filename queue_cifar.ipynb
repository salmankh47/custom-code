{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows an example of input pipeline in tensorflow with CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](http://adventuresinmachinelearning.com/wp-content/uploads/2017/08/AnimatedFileQueues.gif)\n",
    "\n",
    "    1) Read the file names and create a list\n",
    "    \n",
    "    2) Create a Queue to hold and randomly shuffle the filename list\n",
    "    \n",
    "    3) Dequeue files and extract images\n",
    "    \n",
    "    4) Perform image processing\n",
    "    \n",
    "    5) Enqueue new image to a new queue (random shuffle in this case)\n",
    "        \n",
    "    6) Dequeue in batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/cifar/cifar-10-batches-bin/'\n",
    "batch_size = 128\n",
    "num_threads = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1 : Read the file names and create a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all filenames\n",
    "filename_list = [data_path + 'data_batch_{}.bin'.format(i + 1) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 : Create a Queue to hold and randomly shuffle the filename list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq = tf.FIFOQueue(capacity = 10, dtypes=tf.string)\n",
    "\n",
    "# Queue holds tensor strings, hence converting filename_list to tensor\n",
    "string_tensor = tf.convert_to_tensor(filename_list, dtype=tf.string)\n",
    "tf.random_shuffle(string_tensor)  # shuffling just for fun\n",
    "\n",
    "fq_enqueue_op = fq.enqueue_many([string_tensor])\n",
    "\n",
    "# Create queue runner for file queue\n",
    "fq_runner = tf.train.QueueRunner(fq, [fq_enqueue_op] * 1)\n",
    "tf.train.add_queue_runner(fq_runner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 : Dequeue files and extract images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed Length Record Reader, a class provided by tensorflow, handles the dequeuing of the files and reads the data. It returns the processed image and label, with shapes ready for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters required to read a record from data_batch_1 file\n",
    "label_bytes = 1 # 2 for CIFAR-100\n",
    "height = 32\n",
    "width = 32\n",
    "depth = 3\n",
    "image_bytes = height * width * depth  # All these info is avail  # See http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "                                     # for a description of the input format\n",
    "    \n",
    "# Every record consists of a label followed by the image, with a fixed number of bytes\n",
    "record_bytes = image_bytes + label_bytes\n",
    "\n",
    "# Read a record, getting filenames from the filename_queue. Since no header/footer, corresponding options are left default, 0\n",
    "reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "key, value = reader.read(fq)\n",
    "record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "# The first byte represent the label, which we convert from uint8 -> int32\n",
    "#tf.strided_slice(record_bytes, [0], [label_bytes])\n",
    "label = tf.cast(tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "# The remaining bytes are reshaped from [depth * height * width]  to [depth, height, width]\n",
    "depth_major = tf.reshape(tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]), \n",
    "                        [depth, height, width])\n",
    "\n",
    "# Convert from [depth, height, width] to [height, width, depth]\n",
    "img_temp = tf.transpose(depth_major, [1, 2, 0])\n",
    "reshaped_img = tf.cast(img_temp, tf.float32)\n",
    "\n",
    "c_height = 24\n",
    "c_width = 24\n",
    "\n",
    "# Image processing for eval\n",
    "# Crop the central [height, width] of the image\n",
    "resized_img = tf.image.resize_image_with_crop_or_pad(reshaped_img, c_height, c_width)\n",
    "\n",
    "resized_img.set_shape([c_height, c_width, 3])\n",
    "label.set_shape([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding <img, label> pair to random shuffle queue\n",
    "tensor_list = [resized_img, label]\n",
    "dtypes = [tf.float32, tf.int32]\n",
    "shape = [resized_img.get_shape(), label.get_shape()]\n",
    "\n",
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + (num_threads + 1) * batch_size\n",
    "\n",
    "q = tf.RandomShuffleQueue(capacity=capacity, min_after_dequeue=min_after_dequeue, dtypes=dtypes, shapes=shape)\n",
    "\n",
    "enqueue_op = q.enqueue(tensor_list)\n",
    "# add to the queue runner collection\n",
    "tf.train.add_queue_runner(tf.train.QueueRunner(q, [enqueue_op] * num_threads))\n",
    "\n",
    "image_batch, label_batch = q.dequeue_many(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((128, 24, 24, 3), (128, 1))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    \n",
    "    image, lab= sess.run([image_batch, label_batch])\n",
    "    print(image.shape, lab.shape)\n",
    "    \n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow helps to replicate<br> `FIFOQueue`   ::  `string_input_producer` <br>\n",
    "`RandomShuffleQueue` :: `shuffle_batch` <br>\n",
    "<p>\n",
    "    `string_input_producer` takes a list of filenames and creates a FIFOQueue with enqueuing implicitely provided.<br>\n",
    "    `shuffle_batch` creates a RandomShuffleQueue with enqueuing and batch-sized dequeuing already provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = [data_path + 'data_batch_{}.bin'.format(i + 1) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_q = tf.train.string_input_producer(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters required to read a record from data_batch_1 file\n",
    "label_bytes = 1 # 2 for CIFAR-100\n",
    "height = 32\n",
    "width = 32\n",
    "depth = 3\n",
    "image_bytes = height * width * depth  # All these info is avail  # See http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "                                     # for a description of the input format\n",
    "    \n",
    "# Every record consists of a label followed by the image, with a fixed number of bytes\n",
    "record_bytes = image_bytes + label_bytes\n",
    "\n",
    "# Read a record, getting filenames from the filename_queue. Since no header/footer, corresponding options are left default, 0\n",
    "reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "key, value = reader.read(fq)\n",
    "record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "# The first byte represent the label, which we convert from uint8 -> int32\n",
    "#tf.strided_slice(record_bytes, [0], [label_bytes])\n",
    "label = tf.cast(tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "# The remaining bytes are reshaped from [depth * height * width]  to [depth, height, width]\n",
    "depth_major = tf.reshape(tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]), \n",
    "                        [depth, height, width])\n",
    "\n",
    "# Convert from [depth, height, width] to [height, width, depth]\n",
    "img_temp = tf.transpose(depth_major, [1, 2, 0])\n",
    "reshaped_img = tf.cast(img_temp, tf.float32)\n",
    "\n",
    "c_height = 24\n",
    "c_width = 24\n",
    "\n",
    "# Image processing for eval\n",
    "# Crop the central [height, width] of the image\n",
    "resized_img = tf.image.resize_image_with_crop_or_pad(reshaped_img, c_height, c_width)\n",
    "\n",
    "resized_img.set_shape([c_height, c_width, 3])\n",
    "label.set_shape([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_after_dequeue = 10000\n",
    "capacity = min_after_dequeue + (num_threads + 1) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_b, label_b = tf.train.shuffle_batch([image, label],\n",
    "                                          batch_size = batch_size,\n",
    "                                          capacity = capacity,\n",
    "                                          min_after_dequeue = min_after_dequeue,\n",
    "                                          num_threads=num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n",
      "((128, 24, 24, 3), (128, 1))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in range(10):\n",
    "        image, lab= sess.run([image_batch, label_batch])\n",
    "        print(image.shape, lab.shape)\n",
    "    \n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
